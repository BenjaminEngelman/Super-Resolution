{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenjaminEngelman/Super-Resolution/blob/master/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXRJSAVPoJy3",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlVq3dK3kFUJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "900a5721-83a4-45b0-fae1-4af6aa10c85e"
      },
      "source": [
        "!pip install torchvision==0.4.0\n",
        "\n",
        "from __future__ import print_function, division\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision==0.4.0 in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.0) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.0) (4.3.0)\n",
            "Requirement already satisfied: torch==1.2.0 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.0) (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.0) (1.16.4)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision==0.4.0) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA5Ki27soPWv",
        "colab_type": "text"
      },
      "source": [
        "## Google Drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnxmOCHgoEIK",
        "colab_type": "code",
        "outputId": "04d96b8d-840a-44e4-8383-2eee41a277ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_path = 'gdrive/My Drive/videoEnhancer'\n",
        "os.chdir(root_path)  #change dir\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "autoencoder.pt\tframes\tnotebook.ipynb\tvideo1080p.mp4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQuyWBNWoUMv",
        "colab_type": "text"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GzSCYpcj6I6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOW_RES = (540, 960)\n",
        "\n",
        "class FrameDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset in which each element is composed of a frame in 1080p (original)\n",
        "    And the identical frame at 540p (low_res)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root_dir):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the 1080p images.\n",
        "\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.img_names = [name for name in os.listdir(self.root_dir)]\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                                             ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.img_names[idx])\n",
        "        original = io.imread(img_name)\n",
        "        low_res = transform.resize(original, LOW_RES)\n",
        "        sample = {'original': self.transform(original), 'low_res': self.transform(low_res)}\n",
        "\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O7LtCU0PiR2",
        "colab_type": "text"
      },
      "source": [
        "## Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FntSeDZzKeEe",
        "colab_type": "text"
      },
      "source": [
        "### Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BpRqHNcqe2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inspired from: https://github.com/leaxp/Deep-Learning-Super-Resolution-Image-Reconstruction-DSIR/blob/master/conv_autoencoder.py\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "\n",
        "# define the NN architecture\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional Autoencoder. The goal is to start from an Image of low resolution \n",
        "    (960x540) and to transform it into the same image but in a higher resolution\n",
        "    (1920x1080)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 8, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(8)\n",
        "        self.conv3 = nn.Conv2d(8, 8, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(8, 3, 3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(3)\n",
        "        self.convt1 = nn.ConvTranspose2d(8, 8, 2, stride=2)\n",
        "        self.convt2 = nn.ConvTranspose2d(8, 16, 2, stride=2)\n",
        "        self.convt3 = nn.ConvTranspose2d(16, 8, 2, stride=2)\n",
        "        self.convt4 = nn.ConvTranspose2d(8, 16, 2, stride=2)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # Xavier initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.xavier_normal(m.weight.data)\n",
        "                nn.init.normal(m.bias.data)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(F.relu(self.bn1(x)))\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool(F.relu(self.bn2(x))) \n",
        "       \n",
        "        x = self.convt1(x)\n",
        "        x = F.relu(self.bn2(x)) \n",
        "        x = self.convt2(x)\n",
        "        x = F.relu(self.bn1(x)) \n",
        "        x = self.convt3(x)\n",
        "        x = F.relu(self.bn2(x)) \n",
        "        x = self.conv4(x)\n",
        "        x = F.relu(self.bn4(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "# print(model)\n",
        "# summary(model, (3, 540, 960))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chnQLs79KjIg",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3LQ8Tb3Mxhk",
        "colab_type": "text"
      },
      "source": [
        "#### Data setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1ciUgTPMPDb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\n",
        "\n",
        "shuffle_dataset = True\n",
        "dataset = FrameDataset('frames/')\n",
        "validation_split = .2\n",
        "random_seed= 42\n",
        "batch_size = 16\n",
        "\n",
        "# Creating data indices for training and validation splits:\n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "if shuffle_dataset :\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "dataloaders = {\n",
        "    \"train\": DataLoader(dataset, batch_size=batch_size, sampler=train_sampler),\n",
        "    \"val\": DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXFVLT3-M1Bf",
        "colab_type": "text"
      },
      "source": [
        "#### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3eAdRHfP7kr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CHECKPOINT_PATH = \"model/autoencoder.pt\"\n",
        "\n",
        "# initialize the NN\n",
        "model = ConvAutoencoder()\n",
        "model = model.to(device)\n",
        "\n",
        "# specify loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# specify loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# number of epochs to train the model\n",
        "n_epochs = 50\n",
        "best_val_loss = 999\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    # monitor training loss\n",
        "    losses = {'train': 0, 'val': 0}\n",
        "    \n",
        "    for phase in ['train', 'val']:\n",
        "        running_loss = 0\n",
        "        for i, data in enumerate(dataloaders[phase]):\n",
        "            # clear thed gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            inputs = data['low_res'].to(device=device, dtype=torch.float)\n",
        "            originals = data['original'].to(device)\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs = model(inputs)\n",
        "                # calculate the loss\n",
        "                loss = criterion(outputs, originals)\n",
        "                if phase == 'train':\n",
        "                    # backward pass: compute gradient of the loss with respect to model parameters\n",
        "                    loss.backward()\n",
        "                    # perform a single optimization step (parameter update)\n",
        "                    optimizer.step()\n",
        "            # update running training loss\n",
        "            running_loss  += loss.item()*inputs.size(0)\n",
        "            # print(\"Batch %d / %d, loss: %.4f\" % (i + 1, len(dataloader), loss))\n",
        "        losses[phase]  = running_loss / len(dataloaders[phase])\n",
        "    \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch, \n",
        "        losses['train'],\n",
        "        losses['val']\n",
        "        ))\n",
        "    \n",
        "    if losses['val'] < best_val_loss:\n",
        "        best_val_loss = losses['val'] \n",
        "    \n",
        "        torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': losses['train'],\n",
        "                'val_loss': losses['val'],\n",
        "                \n",
        "                }, CHECKPOINT_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkvkjSzYQ_br",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}